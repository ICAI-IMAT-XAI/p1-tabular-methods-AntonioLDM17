{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSpVBPe6W978"
   },
   "source": [
    "# XAI Laboratory 01 - Tabular Methods\n",
    "\n",
    "This notebook is a hands-on laboratory to explore explainable AI (XAI) techniques for tabular datasets.\n",
    "We use the UCI Adult income dataset and an XGBoost classifier as a running example to demonstrate both local and global explanation methods.\n",
    "\n",
    "Sections cover: local explainers (LIME, SHAP), instance-level plots (ICE), global dependence (PDP and ALE), permutation feature importance, and feature interaction using Friedman's H-statistic.\n",
    "Each section includes code to compute the explanation, plots to visualise results, and short guidance on interpretation and caveats.\n",
    "\n",
    "Run the notebook cells sequentially; start by installing the required packages and executing the data/model cells. The examples assume the model and dataset variables (e.g., `X`, `y`, `model`) are already available from earlier cells.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install numpy==1.25.2 pandas==2.0.3 scikit-learn==1.2.2 shap==0.45.1 xgboost==1.7.5 matplotlib==3.7.1 scipy==1.10.1 lime==0.2.0.1\n",
    "!uv pip install git+https://github.com/MaximeJumelle/ALEPython.git@dev#egg=alepython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Models\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# XAI\n",
    "import shap_explainer\n",
    "import lime\n",
    "from alepython import ale_plot # You might need to install from source: pip install git+https://github.com/MaximeJumelle/ALEPython.git@dev#egg=alepython\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "np.random.seed(2025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**\n",
    "We will use the classic [UCI adult income dataset](https://archive.ics.uci.edu/dataset/2/adult). This is a classification task to predict if people made over $50k in the 1990's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data from shap library\n",
    "X, y = shap_explainer.datasets.adult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model**\n",
    "We will train an XGBoost classifier model with default parameters for explanatory purposes. We are also performing a 50-50 train/test split for optimization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train XGBoost model, may take a few minutes\n",
    "model = xgboost.XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Explanations üèòÔ∏è\n",
    "\n",
    "Table of Contents\n",
    "* [LIME üçã](https://colab.research.google.com/drive/1aBZSreILCbF3x7f5PIIlA3eNTmxoyzL7#scrollTo=nTO5RNakdVPW&line=19&uniqifier=1)\n",
    "* [SHAP üé≤](https://colab.research.google.com/drive/1aBZSreILCbF3x7f5PIIlA3eNTmxoyzL7#scrollTo=kRWoDl1SdQbK&line=22&uniqifier=1)\n",
    "* [ICE plots üßä](https://colab.research.google.com/drive/1aBZSreILCbF3x7f5PIIlA3eNTmxoyzL7#scrollTo=8WVOuglNdgEJ&line=9&uniqifier=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME üçã\n",
    "\n",
    "Local interpretable model-agnostic explanations (LIME) [[Paper, 2016]](https://arxiv.org/abs/1602.04938)\n",
    "\n",
    "Interpretable models that are used to explain individual predictions of black box machine learning model\n",
    "\n",
    "**LIME Process:**\n",
    "1.   Select instance of interest\n",
    "2.   Perturb your dataset and get black box predictions for perturbed samples\n",
    "3.   Generate a new dataset consisting of perturbed samples (variations of your data) and the corresponding predictions\n",
    "4.   Train an interpretable model, weighted by the proximity of sampled instances to the instance of interest\n",
    "5.   Interpret the local model to explain prediction\n",
    "\n",
    "As opposed to LIME for text, tabular explainers need a training set. This is because statistics are computed on each feature. If the feature is numerical, we compute the mean and std, and discretize it into quartiles. If the feature is categorical, we compute the frequency of each value. This is used to scale the data, so that we can meaningfully compute distances when the attributes are not on the same scale and to sample perturbed instances - which we do by sampling from a Normal(0,1), multiplying by the std and adding back the mean.\n",
    "\n",
    "\n",
    "LIME uses an exponential smoothing kernel to define the neighborhood.\n",
    "The kernel width determines how large the neighborhood is:\n",
    "* Small kernel width = an instance must be very close to influence the local model\n",
    "* Larger kernel width = instances that are farther away also influence the model\n",
    "\n",
    "[LIME Code Tutorial from original paper authors](https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20continuous%20and%20categorical%20features.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LIME, we need to define some information about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"Age\", \"Workclass\",\n",
    "                 \"Education-Num\", \"Marital Status\", \"Occupation\",\n",
    "                 \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\",\n",
    "                 \"Capital Loss\", \"Hours per week\", \"Country\"]\n",
    "\n",
    "categorical_features = [\"Workclass\", \"Marital Status\", \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Country\"]\n",
    "\n",
    "class_names = ['<=50K', '>50K']\n",
    "\n",
    "categorical_names = {\n",
    "    1: ['Private', 'Self-emp-not-inc', 'Self-emp-inc', 'Federal-gov', 'Local-gov', 'State-gov', 'Without-pay', 'Never-worked'],  # Workclass\n",
    "    3: ['Married-civ-spouse', 'Divorced', 'Never-married', 'Separated', 'Widowed', 'Married-spouse-absent', 'Married-AF-spouse'],  # Marital Status\n",
    "    4: ['Tech-support', 'Craft-repair', 'Other-service', 'Sales', 'Exec-managerial', 'Prof-specialty', 'Handlers-cleaners', 'Machine-op-inspct', 'Adm-clerical', 'Farming-fishing', 'Transport-moving', 'Priv-house-serv', 'Protective-serv', 'Armed-Forces'],  # Occupation\n",
    "    5: ['Wife', 'Own-child', 'Husband', 'Not-in-family', 'Other-relative', 'Unmarried'],  # Relationship\n",
    "    6: ['White', 'Asian-Pac-Islander', 'Amer-Indian-Eskimo', 'Other', 'Black'],  # Race\n",
    "    7: ['Female', 'Male'],  # Sex\n",
    "    11: ['United-States', 'Cambodia', 'England', 'Puerto-Rico', 'Canada', 'Germany', 'Outlying-US(Guam-USVI-etc)', 'India', 'Japan', 'Greece', 'South', 'China', 'Cuba', 'Iran', 'Honduras', 'Philippines', 'Italy', 'Poland', 'Jamaica', 'Vietnam', 'Mexico', 'Portugal', 'Ireland', 'France', 'Dominican-Republic', 'Laos', 'Ecuador', 'Taiwan', 'Haiti', 'Columbia', 'Hungary', 'Guatemala', 'Nicaragua', 'Scotland', 'Thailand', 'Yugoslavia', 'El-Salvador', 'Trinadad&Tobago', 'Peru', 'Hong', 'Holand-Netherlands']  # Country\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define kernel_width\n",
    "kernel_width = 3\n",
    "\n",
    "# Initialize LIME explainer\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values ,class_names=class_names, feature_names=feature_names,\n",
    "                                                   categorical_features=categorical_features,\n",
    "                                                   categorical_names=categorical_names, kernel_width=kernel_width)\n",
    "# Choose a sample for explanation\n",
    "idx = 0\n",
    "\n",
    "# Explain the prediction using LIME\n",
    "exp = explainer.explain_instance(X_test.values[idx], model.predict_proba, num_features=12)\n",
    "\n",
    "# Show the explanation\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Interpret\n",
    "\n",
    "On the left, the prediction probability is shown. This sample was predicted to be <=50K (this individual in the dataset was predicted to make less than $50k)\n",
    "\n",
    "For this sample, features that contributed to the prediction of <=50k are shown in blue (left) and features that contributed to a prediction of >50k are shown in orange (right).\n",
    "\n",
    "The table on the right shows the actual value for each feature for this particular instance and the feature is highlighted with its contribution to each of the binary outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a sample for explanation\n",
    "idx = 100\n",
    "\n",
    "# Explain the prediction using LIME\n",
    "exp = explainer.explain_instance(X_test.values[idx], model.predict_proba, num_features=12)\n",
    "\n",
    "# Show the explanation\n",
    "exp.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Warning about using LIME for tabular data ‚ö†Ô∏è\n",
    "\n",
    "* Defining the neighborhood around a point is difficult and can change the results of your local model\n",
    "\n",
    "* There is not yet a robust way to find the best kernel or width\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP üé≤\n",
    "\n",
    "SHapley Additive exPlanations [Paper, 2017](https://papers.nips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf)\n",
    "\n",
    "\n",
    "Builds off of Shapley Values by approximating them.\n",
    "\n",
    "* Shapley Values is a method from coalitional game theory (where game players cooperate in a coalition) that tells us how to fairly distribute ‚Äúpayout‚Äù to players depending on their contribution to the total payout [Paper, 1952](https://www.rand.org/content/dam/rand/pubs/papers/2021/P295.pdf)\n",
    "* For our purposes, we will assume the prediction task is the ‚Äúgame‚Äù, each feature value of the instance is a ‚Äúplayer‚Äù, and the prediction is the ‚Äúpayout‚Äù\n",
    "\n",
    "The `shap` library has great [documentation](https://shap.readthedocs.io/en/latest/) with excellent tutorials.\n",
    "\n",
    "**The role of the background distribution:**\n",
    "\n",
    "* *Baseline Value Calculation*: SHAP values explain a prediction by comparing it to a baseline value, which is typically the average prediction over the background dataset. This baseline serves as a reference point to understand how much each feature contributes to the difference between the actual prediction and this average prediction. Below, you will see this variable as `X100`.\n",
    "\n",
    "* *Expectation of Model Output*: When calculating SHAP values, the background dataset is used to estimate the expected value of the model's output. This expected value is the average prediction across the background dataset and represents what the model would predict in the absence of specific information about a given instance.\n",
    "\n",
    "* *Marginal Contribution of Features*: SHAP values measure the marginal contribution of each feature to the prediction. This involves considering what the model would predict if each feature were missing or replaced by values from the background distribution. Essentially, SHAP values quantify how much each feature contributes to the difference between the actual prediction and the baseline value derived from the background dataset.\n",
    "\n",
    "* *Sampling and Computational Efficiency*: Using a background dataset allows SHAP to approximate the impact of removing a feature by averaging over many possible values that the feature could take. This is more computationally efficient than considering all possible combinations of feature values.\n",
    "\n",
    "...\n",
    "\n",
    "Below, we implement the **TreeSHAP algorithm**.\n",
    "You can read more about the TreeSHAP algorithm in the [Interpretable ML Book](https://christophm.github.io/interpretable-ml-book/shap.html#treeshap)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values\n",
    "X100 = shap_explainer.utils.sample(X, 100) # 100 instances for use as the background distribution\n",
    "\n",
    "explainer = shap_explainer.TreeExplainer(model, X100) # Use the TreeExplainer algorithm with background distribution\n",
    "shap_values = explainer.shap_values(X_test) # Get shap values\n",
    "shap_values_exp = explainer(X_test) # Get explainer for X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize SHAP with Summary Plot\n",
    "shap_explainer.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Interpret - SHAP Summary Plot\n",
    "\n",
    "The Summary Plot is a beeswarm plot. We can see the features listed on the left side of the plot. The bottom shows the SHAP value, which is the impact on the model output.\n",
    "\n",
    "* For classification models, especially binary classifiers, a negative SHAP value indicates that the feature is contributing to lowering the probability of the positive class, or increasing the probability of the negative class, (<=50k).\n",
    "\n",
    "* Postive SHAP values indicate that the feature is contributing to increasing the probability of the positive class (>50k)\n",
    "\n",
    "* Whether the value of the feature is low or high is shown by the color, with low feature values in blue and high feature values shown in red. This is normalized by feature.\n",
    "\n",
    "* Using the `plot_type=\"bar\"` shows the importance of the input variables as the mean absolute value of the Shapley values of each input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer.summary_plot(shap_values, X_test, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICE Plots üßä\n",
    "\n",
    "Individual Conditional Expectation (ICE) plots one line per instance that displays how the instance‚Äôs prediction changes when a feature changes [Paper, 2014](https://arxiv.org/pdf/1309.6392)\n",
    "\n",
    "**How it Works**\n",
    "1. Select instance and feature of interest\n",
    "2. Keep all other features the same, create variants of the instance by replacing the feature‚Äôs value with values from a grid\n",
    "3. Make predictions with the black box model for newly created instances\n",
    "4. You now have a set of points for an instance with the feature value from the grid and the respective predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the feature of interest\n",
    "features = [\"Age\"]\n",
    "\n",
    "# Use PartialDependenceDisplay to display the ICE plot\n",
    "PartialDependenceDisplay.from_estimator(model, X_test, features, kind='individual') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Centered ICE plots (c-ICE) ‚öñÔ∏è\n",
    "We can center the curves at a certain point in the feature and display only the difference in the prediction to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PartialDependenceDisplay.from_estimator(model, X_test, features, kind='both', centered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ICE Plot using numpy\n",
    "\n",
    "We can also use numpy to compute ICE plots. Scikit-learn does not support looking at individual instances in their PartialDependenceDisplay, so if we want to look at individual instances rather than all of the instances, we will need to code the plots ourselves using numpy and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the instance and feature for which you want to plot the ICE plot\n",
    "instance_index = 0  # Choose the index of the instance you want to visualize\n",
    "feature_index = 0  # Let's look at \"Age\"\n",
    "\n",
    "# Create feature grid\n",
    "feature_values = np.linspace(np.min(X.iloc[:, feature_index]), np.max(X.iloc[:, feature_index]), num=50)\n",
    "\n",
    "# Initialize array to store average predictions\n",
    "average_predictions = np.zeros_like(feature_values)\n",
    "\n",
    "# Extract the instance of interest\n",
    "instance = X.iloc[[instance_index]]\n",
    "\n",
    "# Duplicate the instance to modify feature values\n",
    "instance_modified = instance.copy()\n",
    "\n",
    "# Loop over feature values\n",
    "for i, value in enumerate(feature_values):\n",
    "    # Set the chosen feature to the current value for the instance:\n",
    "    instance_modified.iloc[:, feature_index] = value\n",
    "\n",
    "    # Predict using the modified instance:\n",
    "    prediction = model.predict_proba(instance_modified)[:, 1]\n",
    "\n",
    "    # Store the prediction for the current feature value:\n",
    "    average_predictions[i] = prediction.item()\n",
    "\n",
    "# Plot the ICE plot\n",
    "plt.plot(feature_values, average_predictions)\n",
    "plt.xlabel(f'Feature values')\n",
    "plt.ylabel('Predicted probability of class 1')\n",
    "plt.title(f'ICE Plot - Instance Index: {instance_index}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Interpret - ICE Plot\n",
    "\n",
    "For the instance demonstrated above, as age increases, the predicted probability of >=50k increases. There is interesting behavior around 80 years old for this instance.\n",
    "\n",
    "Try another instance yourself to see how the ICE plot and interpretation changes across instances!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIWpbUcEWnVA"
   },
   "source": [
    "# Global Explanations üåé\n",
    "\n",
    "Table of Contents\n",
    "* [Partial Dependence Plots](https://colab.research.google.com/drive/1WhBODDgVGE4NdnWtCouRuFn8rj0oxT6K#scrollTo=b9WMrsxrSc4Z&line=1&uniqifier=1)\n",
    "* [ALE Plots](https://colab.research.google.com/drive/1WhBODDgVGE4NdnWtCouRuFn8rj0oxT6K#scrollTo=H4na5VBGOath&line=6&uniqifier=1)\n",
    "* [Permutation Feature Importance](https://colab.research.google.com/drive/1WhBODDgVGE4NdnWtCouRuFn8rj0oxT6K#scrollTo=SLIGY1TLbSVm&line=21&uniqifier=1)\n",
    "* [Feature Interaction](https://colab.research.google.com/drive/1WhBODDgVGE4NdnWtCouRuFn8rj0oxT6K#scrollTo=Fn1TE2O-aByw&line=6&uniqifier=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9WMrsxrSc4Z"
   },
   "source": [
    "## Partial Dependence Plots (PDP) üìà\n",
    "\n",
    "A Partial Dependence Plot (PDP or PD) shows the marginal effect one or two features have on the predicted outcome of a model [Paper, 2001](https://jerryfriedman.su.domains/ftp/trebst.pdf)\n",
    "\n",
    "**How it Works:**\n",
    "1. Select feature of interest\n",
    "2. For every instance in training dataset:\n",
    "* Keep all other features the same, create variants of the instance by replacing the feature‚Äôs value with values from a grid\n",
    "* Make predictions with the black box model for newly created instances\n",
    "* You now have a set of points for an instance with the feature value from the grid and the respective predictions\n",
    "3. Average across all instances and plot\n",
    "\n",
    "Here we will show both a built-in library implementation, [scikit learn's Partial Dependence Display](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.PartialDependenceDisplay.html) and we will build our own implementation in numpy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ongpmbktTE6W"
   },
   "source": [
    "#### Using PartialDependenceDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "PzIBxd1eJs_S",
    "outputId": "78bf5e9b-7b74-4b13-aba3-ed74bfd4a5b1"
   },
   "outputs": [],
   "source": [
    "# Choose the feature of interest\n",
    "features = [\"Age\"]\n",
    "\n",
    "# Use PartialDependenceDisplay to plot PDP\n",
    "PartialDependenceDisplay.from_estimator(model, X_test, features, kind='average') #kind='both' if you also want the ICE plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgioG7rlTIUT"
   },
   "source": [
    "#### Build our own PDP with numpy\n",
    "\n",
    "When we build our own, you will notice we build our own grid of values and can change the size of our grid.\n",
    "\n",
    "This is why you may see slight variations between the PDP created with numpy versus the PartialDependenceDisplay from scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "513bNGTyE8Ev",
    "outputId": "4647a4af-4521-4fdb-bac9-982d274c0ba8"
   },
   "outputs": [],
   "source": [
    "# Choose the feature for which you want to plot partial dependence\n",
    "feature_index = 0  # For example, the first feature\n",
    "\n",
    "# Create feature grid - here is where you can update the size of the grid by updating num\n",
    "feature_values = np.linspace(np.min(X.iloc[:, feature_index]), np.max(X.iloc[:, feature_index]), num=80)\n",
    "\n",
    "# Initialize array to store average predictions\n",
    "average_predictions = np.zeros_like(feature_values)\n",
    "\n",
    "# Duplicate the dataset to modify feature values\n",
    "X_modified = X.copy()\n",
    "\n",
    "# Loop over feature values\n",
    "for i, value in enumerate(feature_values):\n",
    "    # Set the chosen feature to the current value for all instances\n",
    "    X_modified.iloc[:, feature_index] = value\n",
    "\n",
    "    # Predict using the modified dataset\n",
    "    predictions = model.predict_proba(X_modified)[:, 1]\n",
    "\n",
    "    # Calculate average prediction for the current feature value\n",
    "    average_predictions[i] = np.mean(predictions)\n",
    "\n",
    "# Plot the partial dependence for the chosen feature\n",
    "plt.plot(feature_values, average_predictions)\n",
    "plt.xlabel(f'Feature {feature_index} values')\n",
    "plt.ylabel('Average predicted probability of class 1')\n",
    "plt.title(f'Partial Dependence for Feature {feature_index}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba4nngrVTLBu"
   },
   "source": [
    "When we build our own PDP, we can run interesting experiments, like the one below, where we can see the impact of changing the size of our grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmxsJ2L0UITe"
   },
   "source": [
    "#### How to Interpret\n",
    "\n",
    "* Look at the shape of the curve on the plot. Is it linear, non-linear, or does it have any particular pattern? This gives you insights into how the feature affects the prediction.\n",
    "\n",
    "* Determine whether increasing or decreasing values of the feature variable lead to higher or lower predictions. Does the curve slope upwards, downwards, or remain relatively flat?\n",
    "\n",
    "* Note whether the curve reaches a plateau or has any upper or lower limits. This indicates whether there's a point beyond which changing the feature variable has little effect on the prediction.\n",
    "\n",
    "* Consider how the observed relationships align with your understanding of the problem domain. Are the results intuitive? Do they make sense based on what you know about the data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "1wFguCx7PU8a",
    "outputId": "39f57dda-892e-4f2d-bc6a-39eecd4c0b07"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_rgba\n",
    "\n",
    "grid_length = np.linspace(20, 120, 40)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))  # Create a figure and axis for plotting\n",
    "\n",
    "# Define a base color and alpha (transparency) values\n",
    "base_color = (0.2, 0.4, 0.6)  # Blue-ish color\n",
    "min_alpha = 0.2\n",
    "max_alpha = 1.0\n",
    "\n",
    "# Normalize grid_length values between 0 and 1\n",
    "normalized_g = (grid_length - grid_length.min()) / (grid_length.max() - grid_length.min())\n",
    "\n",
    "for i, g in enumerate(grid_length):\n",
    "    # Create feature grid\n",
    "    feature_values = np.linspace(np.min(X.iloc[:, feature_index]), np.max(X.iloc[:, feature_index]), num=int(g))\n",
    "    # Initialize array to store average predictions\n",
    "    average_predictions = np.zeros_like(feature_values)\n",
    "    # Duplicate the dataset to modify feature values\n",
    "    X_modified = X.copy()\n",
    "    # Loop over feature values\n",
    "    for j, value in enumerate(feature_values):\n",
    "        # Set the chosen feature to the current value for all instances\n",
    "        X_modified.iloc[:, feature_index] = value\n",
    "        # Predict using the modified dataset\n",
    "        predictions = model.predict_proba(X_modified)[:, 1]\n",
    "        # Calculate average prediction for the current feature value\n",
    "        average_predictions[j] = np.mean(predictions)\n",
    "    # Calculate color based on normalized g\n",
    "    alpha = min_alpha + (max_alpha - min_alpha) * normalized_g[i]\n",
    "    color = to_rgba(base_color, alpha)\n",
    "    # Plot the partial dependence for the chosen feature\n",
    "    ax.plot(feature_values, average_predictions, color=color)\n",
    "\n",
    "ax.set_xlabel(f'Feature {feature_index} values')\n",
    "ax.set_ylabel('Average predicted probability of class 1')\n",
    "ax.set_title(f'Partial Dependence for Feature {feature_index}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4na5VBGOath"
   },
   "source": [
    "## ALE Plots üìâ\n",
    "\n",
    "Accumulated Local Effects (ALE) Plots [Paper, 2020](https://www.scholars.northwestern.edu/en/publications/visualizing-the-effects-of-predictor-variables-in-black-box-super)\n",
    "\n",
    "**How to create an ALE plot:**\n",
    "1. Bin the Feature: Divide the feature of interest into several intervals (bins). These bins help in managing the data and computing local effects within smaller, more manageable segments.\n",
    "2. Compute Local Effects: For each bin, calculate the local effect of the feature on the prediction. This involves: Calculating the change in prediction when moving from the lower to the upper edge of the bin\n",
    "Averaging this change over all instances that fall into that bin\n",
    "3. Accumulate Effects: Starting from the first bin, accumulate the local effects across all bins. Sum up the average effects sequentially to show how the feature influences the prediction as its value changes\n",
    "4. Centering: To make the plot more interpretable, center the accumulated effects around zero. Subtract the mean of the accumulated effects, which forces the interpretation to focus on deviations from the average prediction\n",
    "\n",
    "There are a few python implementations of ALE plots, here we show the [ALEPython implementation](https://github.com/blent-ai/ALEPython).\n",
    "\n",
    "The implementation is more complex and less intuitive than PDPs, with many hyperparameters, including:\n",
    "* **bins** - This parameter defines the number of bins to divide the range of the specified feature into. A larger number of bins can provide more granularity in the ALE plot but might also increase computation time.\n",
    "* **monte_carlo** - This parameter is a boolean flag indicating whether to use Monte Carlo sampling to estimate the ALE. Monte Carlo sampling can be beneficial when the number of samples in the dataset is large, as it reduces computational burden.\n",
    "* **monte_carlo_rep** - This parameter specifies the number of Monte Carlo replicates to use for estimating the ALE. More replicates can lead to a more accurate estimation but may also increase computation time.\n",
    "* **monte_carlo_ratio** - This parameter determines the proportion of the dataset to use for Monte Carlo sampling. It's a value between 0 and 1, where 1 means using the entire dataset. Using a smaller ratio can speed up computation but may introduce some sampling error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAxyYF6VOLDr"
   },
   "source": [
    "This can take a couple of minutes to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "qRnv6MJGNX3A",
    "outputId": "5171e044-be9c-4fd9-e1b2-aacfadb59528"
   },
   "outputs": [],
   "source": [
    "# Use default parameters for 1D Main Effect ALE Plot\n",
    "ale_plot(model, X_train, 'Age', monte_carlo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "NhfPbardQq__",
    "outputId": "8babed21-0f08-40b0-e75a-e21f9a7eda20"
   },
   "outputs": [],
   "source": [
    "# Change hyperparameters for 1D Main Effect ALE Plot\n",
    "ale_plot(\n",
    "    model,\n",
    "    X_train,\n",
    "    \"Age\",\n",
    "    bins=5,\n",
    "    monte_carlo=True,\n",
    "    monte_carlo_rep=30,\n",
    "    monte_carlo_ratio=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWVgjNuaVAQ3"
   },
   "source": [
    "#### How to Interpret 1D Main Effect ALE Plot\n",
    "\n",
    "\n",
    "\n",
    "* X-axis represents feature values\n",
    "* Y-axis shows average effect on predictions\n",
    "* Each curve represents a feature's ALE. - Flat curves imply little impact; steep curves, significant impact\n",
    "* Upward curves: increasing feature value increases predictions; downward, the opposite\n",
    "* Steeper curves signify larger effects\n",
    "\n",
    "We can compare ALE plots to gauge relative feature importance.\n",
    "Features with steeper curves have larger impacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "y3L7Abs6Q5z_",
    "outputId": "72ae9b9d-897b-4046-8b2e-b1f59c9c290e"
   },
   "outputs": [],
   "source": [
    "# 2D Second-Order ALE Plot\n",
    "ale_plot(model, X_train, X_train.columns[:2], bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iR-WV8-fVkDS"
   },
   "source": [
    "#### How to Interpret - 2D Second-Order ALE Plot\n",
    "\n",
    "* Both axes represent the values of the two features being analyzed.\n",
    "* Each axis corresponds to one of the features.\n",
    "* The plot displays a surface where the height represents the average effect on predictions. Higher points indicate regions where the model tends to make higher predictions, and vice versa.\n",
    "* Patterns in the surface reveal how the joint behavior of the two features affects the model's predictions. Peaks or valleys suggest regions where the joint effect is particularly strong.\n",
    "* The direction of the slope indicates whether increasing one feature while holding the other constant tends to increase or decrease predictions. Steeper slopes represent larger effects, while flatter regions indicate smaller effects.\n",
    "\n",
    "\n",
    "We can compare the 2D Second-Order ALE Plot with individual ALE plots for each feature to understand how joint effects differ from marginal effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLIGY1TLbSVm"
   },
   "source": [
    "## Permutation Feature Importance\n",
    "\n",
    "The importance of a feature can be measured by calculating how much model‚Äôs prediction error increases after permuting the feature.\n",
    "* If shuffling a feature‚Äôs values increases the model error, the feature is important\n",
    "* If the model error doesn‚Äôt change after shuffling a feature‚Äôs values, a feature is considered unimportant\n",
    "\n",
    "First introduced for random forests [Paper, 2001](https://link.springer.com/article/10.1023/A:1010933404324). Updated to be model agnostic - renamed ‚Äúmodel reliance‚Äù [Paper, 2018](https://arxiv.org/abs/1801.01489).\n",
    "\n",
    "**Process for Model Agnostic measure:**\n",
    "1. Input: Trained model ùëì^, feature matrix ùëã, target vector ùë¶, error measure ùêø(ùë¶,ùëì^).\n",
    "2. Estimate the original model error ùëíùëúùëüùëñùëî=ùêø(ùë¶,ùëì^(ùëã)) (e.g. mean squared error)\n",
    "3. For each feature ùëó‚àà{1,...,ùëù}:\n",
    "* Generate feature matrix ùëãùëùùëíùëüùëö by permuting feature j in the data X. This breaks the association between feature j and true outcome y\n",
    "* Estimate error ùëíùëùùëíùëüùëö=ùêø(ùëå,ùëì^(ùëãùëùùëíùëüùëö)) based on the predictions of the permuted data.\n",
    "* Calculate permutation feature importance as quotient ùêπùêºùëó=ùëíùëùùëíùëüùëö/ùëíùëúùëüùëñùëî or difference ùêπùêºùëó=ùëíùëùùëíùëüùëö - ùëíùëúùëüùëñùëî\n",
    "4. Sort features by descending FI\n",
    "\n",
    "**Implementation in Python**\n",
    "Here we will demonstrate the implementation in the scikit-learn library.[[Documentation Here](https://scikit-learn.org/stable/modules/permutation_importance.html)]\n",
    "\n",
    "The scikit-learn implementation of the permutation_importance function calculates the feature importance of estimators for a given dataset. The n_repeats parameter sets the number of times a feature is randomly shuffled and returns a sample of feature importances.\n",
    "\n",
    "* n_repeats - number of times to permute a feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dw_Tv4oVcSro"
   },
   "outputs": [],
   "source": [
    "# Compute permutation importances\n",
    "perm_imp = permutation_importance(model, X_test, y_test,\n",
    "                           n_repeats=30,\n",
    "                           random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dptM10KNcxgT",
    "outputId": "4aac5df5-2236-4a1c-b1d4-33310820de9f"
   },
   "outputs": [],
   "source": [
    "# Print the mean and standard deviation of permutation importances for each feature\n",
    "\n",
    "for i in perm_imp.importances_mean.argsort()[::-1]:\n",
    "    if perm_imp.importances_mean[i] - 2 * perm_imp.importances_std[i] > 0:\n",
    "        print(f\"{X.columns[i]:<8}\"\n",
    "              f\"{perm_imp.importances_mean[i]:.3f}\"\n",
    "              f\" +/- {perm_imp.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "CKXg4PsZeZgQ",
    "outputId": "5842d947-32b8-48df-c9b9-a79d54f2cf8a"
   },
   "outputs": [],
   "source": [
    "# Plot Permutation Feature Importances as a bar chart\n",
    "sorted_idx = perm_imp.importances_mean.argsort()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.barh(X_test.columns[sorted_idx], perm_imp.importances[sorted_idx].mean(axis=1).T)\n",
    "ax.set_title(\"Permutation Importances\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "l16dtncffAaZ",
    "outputId": "12788629-d477-490c-b6d7-7a70531b1979"
   },
   "outputs": [],
   "source": [
    "# Plot Permutation Feature Importances as a box plot\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(perm_imp.importances[sorted_idx].T,\n",
    "           vert=False, labels=X_test.columns[sorted_idx])\n",
    "ax.set_title(\"Permutation Importances\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8kN4O0lcITw"
   },
   "source": [
    "‚ö†Ô∏è Caution when using Permutation Feature Importance ‚ö†Ô∏è\n",
    "\n",
    "Features that are deemed of low importance for a bad model (low cross-validation score) could be very important for a good model. Therefore it is always important to evaluate the predictive power of a model using a held-out set (or better with cross-validation) prior to computing importances. Permutation importance does not reflect to the intrinsic predictive value of a feature by itself but how important this feature is for a particular model. [Source](https://scikit-learn.org/stable/modules/permutation_importance.html#id2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Interaction ‚Äî Friedman H statistic\n",
    "\n",
    "We compute Friedman's H-statistic to quantify the interaction strength between pairs of features.\n",
    "We restrict pairwise computations to numeric features for robustness (grid-based averaging on observed values).\n",
    "\n",
    "The implementation below:\n",
    "- approximates the partial dependence on a grid for pairs and single features,\n",
    "- computes H^2 = E[(f_ij - f_i - f_j + f0)^2] / Var(f(X)) and reports H = sqrt(H^2),\n",
    "- displays a heatmap of pairwise H, and lists the top interacting pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Friedman H-statistic (pairwise) - approximate grid-based implementation\n",
    "def friedman_h_pair(model, X, feat_a, feat_b, grid_resolution=10):\n",
    "    # predict probabilities for the positive class and baseline\n",
    "    preds = model.predict_proba(X)[:, 1]\n",
    "    f0 = preds.mean()\n",
    "    denom = np.mean((preds - f0) ** 2)  # Var(f(X)) around mean\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Build grids using quantiles (robust to distribution)\n",
    "    vals_a = np.quantile(X[feat_a], np.linspace(0, 1, grid_resolution))\n",
    "    vals_b = np.quantile(X[feat_b], np.linspace(0, 1, grid_resolution))\n",
    "\n",
    "    # Evaluate f_ij on the grid (mean over other features)\n",
    "    f_ij = np.zeros((len(vals_a), len(vals_b)))\n",
    "    for i, va in enumerate(vals_a):\n",
    "        for j, vb in enumerate(vals_b):\n",
    "            X_temp = X.copy()\n",
    "            X_temp[feat_a] = va\n",
    "            X_temp[feat_b] = vb\n",
    "            f_ij[i, j] = model.predict_proba(X_temp)[:, 1].mean()\n",
    "\n",
    "    # Evaluate main effects on their grids\n",
    "    f_i = np.zeros(len(vals_a))\n",
    "    for i, va in enumerate(vals_a):\n",
    "        X_temp = X.copy(); X_temp[feat_a] = va\n",
    "        f_i[i] = model.predict_proba(X_temp)[:, 1].mean()\n",
    "\n",
    "    f_j = np.zeros(len(vals_b))\n",
    "    for j, vb in enumerate(vals_b):\n",
    "        X_temp = X.copy(); X_temp[feat_b] = vb\n",
    "        f_j[j] = model.predict_proba(X_temp)[:, 1].mean()\n",
    "\n",
    "    # Map each instance to nearest grid cell (by absolute distance)\n",
    "    a_vals = X[feat_a].values\n",
    "    b_vals = X[feat_b].values\n",
    "    idx_a = np.abs(a_vals[:, None] - vals_a[None, :]).argmin(axis=1)\n",
    "    idx_b = np.abs(b_vals[:, None] - vals_b[None, :]).argmin(axis=1)\n",
    "\n",
    "    f_ij_at = f_ij[idx_a, idx_b]\n",
    "    f_i_at = f_i[idx_a]\n",
    "    f_j_at = f_j[idx_b]\n",
    "\n",
    "    diff = f_ij_at - f_i_at - f_j_at + f0\n",
    "    num = np.mean(diff ** 2)\n",
    "    h2 = num / denom if denom > 0 else 0.0\n",
    "    return float(np.sqrt(max(0.0, h2)))\n",
    "\n",
    "# Compute pairwise H for numeric features only (safer for grid-based approach)\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "n = len(numeric_cols)\n",
    "H_mat = np.zeros((n, n))\n",
    "\n",
    "for i, j in itertools.combinations(range(n), 2):\n",
    "    a = numeric_cols[i]\n",
    "    b = numeric_cols[j]\n",
    "    h = friedman_h_pair(model, X, a, b, grid_resolution=10)\n",
    "    H_mat[i, j] = H_mat[j, i] = h\n",
    "\n",
    "# Create a DataFrame for easier display\n",
    "H_df = pd.DataFrame(H_mat, index=numeric_cols, columns=numeric_cols)\n",
    "\n",
    "# Show heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(H_df, annot=True, fmt='.2f', cmap='viridis', square=True, cbar_kws={'label': 'H stat'})\n",
    "plt.title('Friedman H-statistic (pairwise) - numeric features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top interacting pairs\n",
    "pairs = []\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        pairs.append((numeric_cols[i], numeric_cols[j], H_mat[i, j]))\n",
    "pairs_sorted = sorted(pairs, key=lambda x: x[2], reverse=True)\n",
    "print('Top interacting pairs (feature_a, feature_b, H):')\n",
    "for a, b, h in pairs_sorted[:10]:\n",
    "    print(f'{a:<20} {b:<20} {h:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to interpret the Friedman H-statistic\n",
    "\n",
    "- The H-statistic measures the fraction of the model output variance that is attributable to interaction between the two features (for pairwise H). Practically, H is the square-root of that fraction, so H in [0, 1].\n",
    "- H ‚âà 0: little to no interaction ‚Äî the joint effect of the two features is well explained by the sum of their individual (main) effects.\n",
    "- H closer to 1: strong interaction ‚Äî a large portion of the model's variance comes from the joint behaviour of the two features (their combined effect cannot be decomposed into independent main effects).\n",
    "- Compare H across pairs: higher H indicates stronger interaction relative to other pairs, but absolute values depend on model output scale and baseline variance; use rankings rather than single-number thresholds when in doubt.\n",
    "- Caveats and practical tips:\n",
    "  - Our implementation uses grid-based averages (quantile grids) and numeric features; results depend on grid_resolution and the grid strategy. Increase resolution for more accurate estimates (at higher computational cost).\n",
    "  - For categorical features, evaluate over observed categories rather than numeric quantiles (not implemented above).\n",
    "  - H is model- and data-dependent: different models or different background datasets can change H values. Always compute H using the dataset of interest (e.g., training or validation set).\n",
    "  - Small sample sizes, uneven distributions, or coarse grids can produce noisy H estimates ‚Äî treat small differences with caution.\n",
    "\n",
    "Use the heatmap and the printed top pairs to identify candidate interacting features, then inspect 2D PDPs, ALE surfaces, or conditional dependence plots for those pairs to understand the interaction shape and domain regions where it matters."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Laboratory 01 - Tabular Explanation Methods",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
